{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bfd4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import copy\n",
    "from tpot.config import classifier_config_dict_light\n",
    "import fasttext.util\n",
    "import string\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import balanced_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5484bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NIC GP data.csv').iloc[:, 4:]\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311cd244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>bot?</th>\n",
       "      <th>number_likes</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>False</td>\n",
       "      <td>30634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>False</td>\n",
       "      <td>64192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>False</td>\n",
       "      <td>1396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>False</td>\n",
       "      <td>24049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>False</td>\n",
       "      <td>13076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>I am visiting Sri Lanka soonfor 9 days, how ca...</td>\n",
       "      <td>Do Indians hate Sri Lankans?</td>\n",
       "      <td>False</td>\n",
       "      <td>24454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>What are some good examples of 4 stanza poems?</td>\n",
       "      <td>What are some good Ilocano poems?</td>\n",
       "      <td>False</td>\n",
       "      <td>2611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Which CPU is better I3 4th Gen or 6th Gen?</td>\n",
       "      <td>Which is better intel i5 (6th gen) or i7 (5th ...</td>\n",
       "      <td>True</td>\n",
       "      <td>18483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>What are some of the best tourist places to vi...</td>\n",
       "      <td>Where are the foremost tourist places in Chhat...</td>\n",
       "      <td>True</td>\n",
       "      <td>28018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>What are the differences between a love marria...</td>\n",
       "      <td>Which is better: an arranged marriage or a lov...</td>\n",
       "      <td>False</td>\n",
       "      <td>20559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question1  \\\n",
       "0     What is the step by step guide to invest in sh...   \n",
       "1     What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2     How can I increase the speed of my internet co...   \n",
       "3     Why am I mentally very lonely? How can I solve...   \n",
       "4     Which one dissolve in water quikly sugar, salt...   \n",
       "...                                                 ...   \n",
       "1995  I am visiting Sri Lanka soonfor 9 days, how ca...   \n",
       "1996     What are some good examples of 4 stanza poems?   \n",
       "1997         Which CPU is better I3 4th Gen or 6th Gen?   \n",
       "1998  What are some of the best tourist places to vi...   \n",
       "1999  What are the differences between a love marria...   \n",
       "\n",
       "                                              question2   bot?  number_likes  \\\n",
       "0     What is the step by step guide to invest in sh...  False         30634   \n",
       "1     What would happen if the Indian government sto...  False         64192   \n",
       "2     How can Internet speed be increased by hacking...  False          1396   \n",
       "3     Find the remainder when [math]23^{24}[/math] i...  False         24049   \n",
       "4               Which fish would survive in salt water?  False         13076   \n",
       "...                                                 ...    ...           ...   \n",
       "1995                       Do Indians hate Sri Lankans?  False         24454   \n",
       "1996                  What are some good Ilocano poems?  False          2611   \n",
       "1997  Which is better intel i5 (6th gen) or i7 (5th ...   True         18483   \n",
       "1998  Where are the foremost tourist places in Chhat...   True         28018   \n",
       "1999  Which is better: an arranged marriage or a lov...  False         20559   \n",
       "\n",
       "      is_duplicate  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "1995             0  \n",
       "1996             0  \n",
       "1997             0  \n",
       "1998             1  \n",
       "1999             0  \n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd868ac",
   "metadata": {},
   "source": [
    "# Create several encoders\n",
    "Since we have text attributes in our dataset, we need to transform them into floating point vectors.\n",
    "Here I create three encoders:\n",
    "* TF-IDF\n",
    "* FastText\n",
    "* BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab9c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionsToFloats(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.model = TfidfVectorizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        corpus = X.question1.tolist() + X.question2.tolist()\n",
    "        corpus = list(map(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)), corpus))\n",
    "        self.model.fit(corpus)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        q1, q2 = X.question1, X.question2\n",
    "        q1_vec, q2_vec = self.model.transform(q1), self.model.transform(q2)\n",
    "        q1_vec, q2_vec = q1_vec.todense(), q2_vec.todense()\n",
    "\n",
    "        return np.concatenate((q1_vec, q2_vec, \n",
    "                               X['bot?'].to_numpy()[..., None], \n",
    "                               X['number_likes'].to_numpy()[..., None]), \n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b9671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Use --overwrite to download anyway.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext.util.download_model('en')  # English\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7b0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEncoding(TransformerMixin, BaseEstimator):      \n",
    "    def fit(self, X, y=None):       \n",
    "        return self\n",
    "    \n",
    "    def _helper(self, text):\n",
    "        words = text.split()\n",
    "        words = list(map(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)), words))\n",
    "        words = list(map(fasttext_model.get_word_vector, words))\n",
    "        \n",
    "        return np.mean(words, axis=0)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        q1, q2 = X.question1, X.question2\n",
    "        q1_vec = q1.apply(self._helper)\n",
    "        q1_vec = pd.DataFrame.from_dict(dict(zip(q1_vec.index, q1_vec.values)))\n",
    "        q2_vec = q2.apply(self._helper)\n",
    "        q2_vec = pd.DataFrame.from_dict(dict(zip(q2_vec.index, q2_vec.values)))\n",
    "\n",
    "        return np.concatenate((q1_vec.T.to_numpy(), q2_vec.T.to_numpy(), \n",
    "                               X['bot?'].to_numpy()[..., None], \n",
    "                               X['number_likes'].to_numpy()[..., None]), \n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0bf877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0a0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of code are taken from here: http://mccormickml.com/2019/07/22/BERT-fine-tuning/#32-required-formatting\n",
    "# and here: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#1-loading-pre-trained-bert\n",
    "\n",
    "class BertEncoding(TransformerMixin, BaseEstimator):      \n",
    "    def fit(self, X, y=None):       \n",
    "        return self\n",
    "    \n",
    "    def _helper(self, text):\n",
    "        sentences = text.split('.')\n",
    "        ids = list(map(lambda sent: tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS]' + sent + '[SEP]')), \n",
    "                       sentences))\n",
    "        \n",
    "        ids = list(map(torch.tensor, ids))\n",
    "        tokens_tensor = torch.concat(ids).unsqueeze(0).cuda()\n",
    "        segments_tensors = torch.ones(len(tokens_tensor)).unsqueeze(0).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            hidden_states = outputs[2]\n",
    "            \n",
    "            token_vecs = hidden_states[-2][0]\n",
    "\n",
    "            sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "            \n",
    "            return sentence_embedding.cpu()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        q1, q2 = X.question1, X.question2\n",
    "        q1_vec = q1.apply(self._helper)\n",
    "        q1_vec = pd.DataFrame.from_dict(dict(zip(q1_vec.index, q1_vec.values)))\n",
    "        q2_vec = q2.apply(self._helper)\n",
    "        q2_vec = pd.DataFrame.from_dict(dict(zip(q2_vec.index, q2_vec.values)))\n",
    "\n",
    "        return np.concatenate((q1_vec.T.to_numpy(), q2_vec.T.to_numpy(), \n",
    "                               X['bot?'].to_numpy()[..., None], \n",
    "                               X['number_likes'].to_numpy()[..., None]), \n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928e98c",
   "metadata": {},
   "source": [
    "It is the easiest way to put all encoders into tpot. I have a single Encoder class with parameter 'encoder_type'. This parameter controls which specific encoder will be used for transformation of the dataset. The task of Tpot is to choose the best performing option for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb6539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, encoder_type):\n",
    "        self.encoder_type = encoder_type\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.encoder_type == 'qtf':\n",
    "            self.model = QuestionsToFloats()\n",
    "        elif self.encoder_type == 'fte':\n",
    "            self.model = FastTextEncoding()\n",
    "        elif self.encoder_type == 'be':\n",
    "            self.model = BertEncoding()\n",
    "            \n",
    "        return self.model.fit(X, y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2c9a",
   "metadata": {},
   "source": [
    "Add encoders to the tpot config. Tpot is required to choose the best out of three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8fa3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TPOT config\n",
    "config = copy.deepcopy(classifier_config_dict_light)\n",
    "config[\"__main__.Encoder\"] = {'encoder_type': ['qtf', 'fte', 'be']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb6bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2,\n",
    "                                    scoring='balanced_accuracy',\n",
    "                                    config_dict=config,\n",
    "                                    periodic_checkpoint_folder='ckpt',\n",
    "                                    template='Encoder-Selector-Transformer-Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a058cf6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.6339092382386959\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.6339092382386959\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.6339092382386959\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.6339092382386959\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.6339092382386959\n",
      "\n",
      "Best pipeline: GaussianNB(StandardScaler(VarianceThreshold(Encoder(input_matrix, encoder_type=be), threshold=0.0001)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TPOTClassifier(config_dict={&#x27;__main__.Encoder&#x27;: {&#x27;encoder_type&#x27;: [&#x27;qtf&#x27;, &#x27;fte&#x27;,\n",
       "                                                                  &#x27;be&#x27;]},\n",
       "                            &#x27;sklearn.cluster.FeatureAgglomeration&#x27;: {&#x27;affinity&#x27;: [&#x27;euclidean&#x27;,\n",
       "                                                                                  &#x27;l1&#x27;,\n",
       "                                                                                  &#x27;l2&#x27;,\n",
       "                                                                                  &#x27;manhattan&#x27;,\n",
       "                                                                                  &#x27;cosine&#x27;],\n",
       "                                                                     &#x27;linkage&#x27;: [&#x27;ward&#x27;,\n",
       "                                                                                 &#x27;complete&#x27;,\n",
       "                                                                                 &#x27;average&#x27;]},\n",
       "                            &#x27;sklearn.decomposition.PCA&#x27;: {&#x27;iterated_power&#x27;: range(1, 11),\n",
       "                                                          &#x27;svd_solver&#x27;: [&#x27;randomized&#x27;]},\n",
       "                            &#x27;sklearn.feature_selection.SelectFwe&#x27;: {&#x27;alp...\n",
       "                            &#x27;sklearn.tree.DecisionTreeClassifier&#x27;: {&#x27;criterion&#x27;: [&#x27;gini&#x27;,\n",
       "                                                                                  &#x27;entropy&#x27;],\n",
       "                                                                    &#x27;max_depth&#x27;: range(1, 11),\n",
       "                                                                    &#x27;min_samples_leaf&#x27;: range(1, 21),\n",
       "                                                                    &#x27;min_samples_split&#x27;: range(2, 21)},\n",
       "                            &#x27;tpot.builtins.ZeroCount&#x27;: {}},\n",
       "               generations=5, periodic_checkpoint_folder=&#x27;ckpt&#x27;,\n",
       "               population_size=20, random_state=42, scoring=&#x27;balanced_accuracy&#x27;,\n",
       "               template=&#x27;Encoder-Selector-Transformer-Classifier&#x27;, verbosity=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TPOTClassifier</label><div class=\"sk-toggleable__content\"><pre>TPOTClassifier(config_dict={&#x27;__main__.Encoder&#x27;: {&#x27;encoder_type&#x27;: [&#x27;qtf&#x27;, &#x27;fte&#x27;,\n",
       "                                                                  &#x27;be&#x27;]},\n",
       "                            &#x27;sklearn.cluster.FeatureAgglomeration&#x27;: {&#x27;affinity&#x27;: [&#x27;euclidean&#x27;,\n",
       "                                                                                  &#x27;l1&#x27;,\n",
       "                                                                                  &#x27;l2&#x27;,\n",
       "                                                                                  &#x27;manhattan&#x27;,\n",
       "                                                                                  &#x27;cosine&#x27;],\n",
       "                                                                     &#x27;linkage&#x27;: [&#x27;ward&#x27;,\n",
       "                                                                                 &#x27;complete&#x27;,\n",
       "                                                                                 &#x27;average&#x27;]},\n",
       "                            &#x27;sklearn.decomposition.PCA&#x27;: {&#x27;iterated_power&#x27;: range(1, 11),\n",
       "                                                          &#x27;svd_solver&#x27;: [&#x27;randomized&#x27;]},\n",
       "                            &#x27;sklearn.feature_selection.SelectFwe&#x27;: {&#x27;alp...\n",
       "                            &#x27;sklearn.tree.DecisionTreeClassifier&#x27;: {&#x27;criterion&#x27;: [&#x27;gini&#x27;,\n",
       "                                                                                  &#x27;entropy&#x27;],\n",
       "                                                                    &#x27;max_depth&#x27;: range(1, 11),\n",
       "                                                                    &#x27;min_samples_leaf&#x27;: range(1, 21),\n",
       "                                                                    &#x27;min_samples_split&#x27;: range(2, 21)},\n",
       "                            &#x27;tpot.builtins.ZeroCount&#x27;: {}},\n",
       "               generations=5, periodic_checkpoint_folder=&#x27;ckpt&#x27;,\n",
       "               population_size=20, random_state=42, scoring=&#x27;balanced_accuracy&#x27;,\n",
       "               template=&#x27;Encoder-Selector-Transformer-Classifier&#x27;, verbosity=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TPOTClassifier(config_dict={'__main__.Encoder': {'encoder_type': ['qtf', 'fte',\n",
       "                                                                  'be']},\n",
       "                            'sklearn.cluster.FeatureAgglomeration': {'affinity': ['euclidean',\n",
       "                                                                                  'l1',\n",
       "                                                                                  'l2',\n",
       "                                                                                  'manhattan',\n",
       "                                                                                  'cosine'],\n",
       "                                                                     'linkage': ['ward',\n",
       "                                                                                 'complete',\n",
       "                                                                                 'average']},\n",
       "                            'sklearn.decomposition.PCA': {'iterated_power': range(1, 11),\n",
       "                                                          'svd_solver': ['randomized']},\n",
       "                            'sklearn.feature_selection.SelectFwe': {'alp...\n",
       "                            'sklearn.tree.DecisionTreeClassifier': {'criterion': ['gini',\n",
       "                                                                                  'entropy'],\n",
       "                                                                    'max_depth': range(1, 11),\n",
       "                                                                    'min_samples_leaf': range(1, 21),\n",
       "                                                                    'min_samples_split': range(2, 21)},\n",
       "                            'tpot.builtins.ZeroCount': {}},\n",
       "               generations=5, periodic_checkpoint_folder='ckpt',\n",
       "               population_size=20, random_state=42, scoring='balanced_accuracy',\n",
       "               template='Encoder-Selector-Transformer-Classifier', verbosity=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa3cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c63f4bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6373134328358209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load tpot_exported_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __main__ import Encoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "# # NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(features, tpot_data['target'], random_state=42)\n",
    "\n",
    "# Average CV score on the training set was: 0.6339092382386959\n",
    "exported_pipeline = make_pipeline(\n",
    "    Encoder(encoder_type=\"be\"),\n",
    "    VarianceThreshold(threshold=0.0001),\n",
    "    StandardScaler(),\n",
    "    GaussianNB()\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)\n",
    "balanced_accuracy_score(y_test, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nic]",
   "language": "python",
   "name": "conda-env-nic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
